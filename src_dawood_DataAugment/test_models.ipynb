{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from utils import device\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Policy, self).__init__()\n",
    "        lstm_dim = 256 + config[\"proprio_dim\"] #config[\"visual_embedding_dim\"] -> 8192 = 16x16x32 -> 4096 (8x8x64) --> 2048 (8x8x32) --> 4096 (16x16x16) --> 2048 (16x16x8)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=4, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=4, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=8, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=8, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=16, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "        self.lstm = nn.LSTM(lstm_dim, lstm_dim)  # , batch_first=True)\n",
    "        self.linear_out = nn.Linear(lstm_dim, config[\"action_dim\"])\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=config[\"learning_rate\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "        )\n",
    "        self.std = 0.1 * torch.ones(config[\"action_dim\"], dtype=torch.float32)\n",
    "        self.std = self.std.to(device)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        return\n",
    "\n",
    "    def forward_step(self, camera_obs, proprio_obs, lstm_state):\n",
    "        vis_encoding = F.elu(self.conv1(camera_obs))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = F.elu(self.conv2(vis_encoding))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = F.elu(self.conv3(vis_encoding))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = F.elu(self.conv4(vis_encoding))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = F.elu(self.conv5(vis_encoding))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = F.elu(self.conv6(vis_encoding))\n",
    "        vis_encoding = self.maxpool(vis_encoding)\n",
    "        vis_encoding = torch.flatten(vis_encoding, start_dim=1)\n",
    "        low_dim_input = torch.cat((vis_encoding, proprio_obs), dim=-1).unsqueeze(0)\n",
    "        low_dim_input = self.dropout(low_dim_input)\n",
    "        lstm_out, (h, c) = self.lstm(low_dim_input, lstm_state)\n",
    "        lstm_state = (h, c)\n",
    "        out = torch.tanh(self.linear_out(lstm_out))\n",
    "        return out, lstm_state\n",
    "\n",
    "    def forward(self, camera_obs_traj, proprio_obs_traj, action_traj, feedback_traj):\n",
    "        losses = []\n",
    "        lstm_state = None\n",
    "        for idx in range(len(proprio_obs_traj)):\n",
    "            mu, lstm_state = self.forward_step(\n",
    "                camera_obs_traj[idx], proprio_obs_traj[idx], lstm_state\n",
    "            )\n",
    "            distribution = Normal(mu, self.std)\n",
    "            log_prob = distribution.log_prob(action_traj[idx])\n",
    "            loss = -log_prob * feedback_traj[idx]\n",
    "            losses.append(loss)\n",
    "        total_loss = torch.cat(losses).mean()\n",
    "        return total_loss\n",
    "\n",
    "    def update_params(\n",
    "        self, camera_obs_traj, proprio_obs_traj, action_traj, feedback_traj\n",
    "    ):\n",
    "        camera_obs = camera_obs_traj.to(device)\n",
    "        proprio_obs = proprio_obs_traj.to(device)\n",
    "        action = action_traj.to(device)\n",
    "        feedback = feedback_traj.to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward(camera_obs, proprio_obs, action, feedback)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        training_metrics = {\"loss\": loss}\n",
    "        return training_metrics\n",
    "\n",
    "    def predict(self, camera_obs, proprio_obs, lstm_state):\n",
    "        camera_obs_th = torch.tensor(camera_obs, dtype=torch.float32).unsqueeze(0)\n",
    "        proprio_obs_th = torch.tensor(proprio_obs, dtype=torch.float32).unsqueeze(0)\n",
    "        camera_obs_th = camera_obs_th.to(device)\n",
    "        proprio_obs_th = proprio_obs_th.to(device)\n",
    "        with torch.no_grad():\n",
    "            action_th, lstm_state = self.forward_step(\n",
    "                camera_obs_th, proprio_obs_th, lstm_state\n",
    "            )\n",
    "            action = action_th.detach().cpu().squeeze(0).squeeze(0).numpy()\n",
    "            action[-1] = binary_gripper(action[-1])\n",
    "        return action, lstm_state\n",
    "\n",
    "\n",
    "def binary_gripper(gripper_action):\n",
    "    if gripper_action >= 0.0:\n",
    "        gripper_action = 0.9\n",
    "    elif gripper_action < 0.0:\n",
    "        gripper_action = -0.9\n",
    "    return gripper_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"feedback_type\": 'cloning_100',\n",
    "        \"task\": 'stator_100',\n",
    "        \"proprio_dim\": 8,\n",
    "        \"action_dim\": 7,\n",
    "        \"visual_embedding_dim\": 256,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"weight_decay\": 3e-6,\n",
    "        \"batch_size\": 8,#16\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566163\n"
     ]
    }
   ],
   "source": [
    "policy = Policy(config).to(device)\n",
    "    \n",
    "pytorch_total_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = torch.load(\"/home/faps/CEILing/CEILing256_v2/data/stators_100/demos_100.dat\")\n",
    "batch = replay_memory.sample(config[\"batch_size\"])\n",
    "camera_batch, proprio_batch, action_batch, feedback_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(8,3,256,256)\n",
    "\n",
    "# input_tensor = preprocess(input_image)\n",
    "\n",
    "input_batch = input_tensor\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=4, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "conv2 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=4, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "conv3 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=8, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "conv4 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=8, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "conv5 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "conv6 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=16, kernel_size=3, padding=2, stride=1\n",
    "        )\n",
    "\n",
    "maxpool = nn.MaxPool2d(kernel_size=3,stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv1(input_batch)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv2(vis_encoding)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv3(vis_encoding)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv4(vis_encoding)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv5(vis_encoding)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "vis_encoding = conv6(vis_encoding)\n",
    "vis_encoding = maxpool(vis_encoding)\n",
    "print(vis_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(13.8156, device='cuda', grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics = policy.update_params(\n",
    "            camera_batch, proprio_batch, action_batch, feedback_batch\n",
    "        )\n",
    "\n",
    "\n",
    "training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|      Modules      | Parameters |\n",
      "+-------------------+------------+\n",
      "|    conv1.weight   |    108     |\n",
      "|     conv1.bias    |     4      |\n",
      "|    conv2.weight   |    144     |\n",
      "|     conv2.bias    |     4      |\n",
      "|    conv3.weight   |    288     |\n",
      "|     conv3.bias    |     8      |\n",
      "|    conv4.weight   |    576     |\n",
      "|     conv4.bias    |     8      |\n",
      "|    conv5.weight   |    1152    |\n",
      "|     conv5.bias    |     16     |\n",
      "|    conv6.weight   |    2304    |\n",
      "|     conv6.bias    |     16     |\n",
      "| lstm.weight_ih_l0 |   278784   |\n",
      "| lstm.weight_hh_l0 |   278784   |\n",
      "|  lstm.bias_ih_l0  |    1056    |\n",
      "|  lstm.bias_hh_l0  |    1056    |\n",
      "| linear_out.weight |    1848    |\n",
      "|  linear_out.bias  |     7      |\n",
      "+-------------------+------------+\n",
      "Total Trainable Params: 566163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "566163"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceiling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
